%! suppress = TooLargeSection
%%
%% This is file `sample-lualatex.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-lualatex.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{color, soul}
\hypersetup{colorlinks=true,linkcolor=blue}
\geometry{a4paper}
%\citestyle{acmauthoryear}

% for comments
\newcommand{\mpu}[1]{\hl{\textbf{MP:} #1}}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{rightsretained}
\copyrightyear{2022}
%\acmYear{2018}
\acmDOI{}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Information Society 2021]{Information Society 2021: 24th international multiconference}{4--8 October 2021}{Ljubljana, Slovenia}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmPrice{}
\acmISBN{}
\settopmatter{printccs=false, printacmref=false}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Analysis of Transfer Learning for Named Entity Recognition in South-Slavic Languages}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Nikola Ivačič}
%\authornote{Both authors contributed equally to this research.}
% \email{nikola.ivacic@gmail.com}
%\orcid{1234-5678-9012}
%\author{G.K.M. Tobin}
%\authornotemark[1]
%\email{webmaster@marysville-ohio.com}


\author{Hanh Tran}
\author{Boshko Koloski}
\author{Senja Pollak}
\author{Marko Pranjić}
\author{Nada Lavrač}
\affiliation{
  \institution{Jožef Stefan Institute\\
  Jamova cesta 39}
  \streetaddress{Jamova cesta 39}
  \city{Ljubljana}
  \country{Slovenia}
  \postcode{1000}
}

\author{Matthew Purver}
\affiliation{
  \institution{School of Electronic Engineering and Computer Science\\
  Queen Mary University of London}
  \streetaddress{Mile End Road\\
   London E1 4NS, UK}
  \city{London}
  \country{UK}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Ivačič et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  This paper analyzes a Named Entity Recognition task for South-Slavic languages using the pre-trained multilingual neural network models.
  Observing performance metrics from prior research showed that the performance of the fine-tuned multilingual neural model is very close to the performance of the monolingual one.
  This observation leads us to a question that this paper aims to answer: Can the fine-tuning of multilingual pre-trained embeddings, without using the target dataset itself, improve named entity recognition for a specific language?
  We show that the model performance is not influenced substantially, especially if the target language has rich Named Entity training corpora.
\end{abstract}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{natural language processing, named entity recognition, neural networks, text classification, multilingual pre-trained models}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
%% NikComm\begin{teaserfigure}
%% NikComm  \includegraphics[width=\textwidth]{sampleteaser}
%% NikComm  \caption{Seattle Mariners at Spring Training, 2010.}
%% NikComm  \Description{Enjoying the baseball game from the third-base
%% NikComm  seats. Ichiro Suzuki preparing to bat.}
%% NikComm  \label{fig:teaser}
%% NikComm\end{teaserfigure}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:introduction}
Multilingual neural models simplify scalability and applications where many languages are needed for the Natural Language Processing (NLP) task.
Getting the performance of a multilingual neural model as close as possible to the performance of a monolingual one can be very beneficial.
Named Entity Recognition (NER) is also one of the cornerstones of the NLP tasks in many text processing systems.

We observed two prior works:
\begin{itemize}
\item BSNLP: 3rd Shared Task on SlavNER~\cite{piskorski-etal-2021-slav} submitted paper by Prelevikj and Žitnik et al.~\cite{prelevikj-zitnik-2021-multilingual}
\item Clipping industry project developed by the Department of Knowledge Technologies at Jožef Stefan Institute~\cite{KTIJS}
\end{itemize}
The prior work mentioned above has shown that the monolingual NER model performance for the Slovene language is practically equal to that of a multilingual one.
Additionally, the Clipping industry project multilingual model was fine-tuned with other language corpora from the same - South-Slavic family.

The question that naturally arises from prior work and this paper tries to answer is this: Did the fine-tuning with related languages influence the performance of a multilingual model compared to fine-tuning only in the target language?
The problem with comparing both prior works is that the: corpus preprocessing, pre-trained models, hyper-parameters, and performance measures were different in both experiments, so one has to be cautious when evaluating the performance metrics or deriving conclusions from them.

First, we present the corpora we used and how we preprocessed them, followed by their analysis.
Next, we continue with a method where we first introduce the measures, models, hyper-parameters, and software used.
Finally, we finish this paper by evaluating the results and presenting conclusions.

\section{Data Description}
\label{sec:data-description}
In this section, we first present all the corpora used. Then, we continue with the description of the conversion of these datasets to the expected format and conclude with the corpora structure analysis.

We used the most common and established NER corpora for selected languages (see Table~\ref{tab:corpora}).
The assumption and strategy for gathering corpora were also: ``the more, the better.''
%\begin{table}[H]
  %\caption{List of Used Corpora \hl{define what each column represents -}}
  %\label{tab:corpora}
  %\begin{tabular}{llrrl}
    %\toprule
    %Lang.&Abbr.&Sentences&Tokens&Name\\
    %\midrule

    %\multicolumn{4}{}{}
    %sl&bsnlp&18106&400291&BSNLP 2017/21~\cite{piskorski-etal-2021-slav}\\
    %sl&500k&9483&193611&ssj500k 2.3~\cite{ssj500k-23}\\
    %sl&ewsd&2024&31233&{ELEXIS}-{WSD} 1.0~\cite{ELEXIS-WSD-10}\\
    %sl&scr&18139&391526&{SentiCoref} 1.0~\cite{SentiCoref-10}\\
    %\midrule
    %hr&bsnlp&820&18704&BSNLP 2017 and 2021~\cite{piskorski-etal-2021-slav}\\
    %hr&500k&24780&504227&hr500k 1.0 ~ \cite{hr500k-10}\\
    %\midrule
    %sr&set&3891&86726&{SETimes}.{SR} 1.0~\cite{SETimes-SR-1.0}\\
    %\midrule
    %bs&wann&8917&199378&WikiANN - PAN-X~\cite{rahimi-etal-2019-massively}\\
    %\midrule
    %mk&wann&16227&156467&WikiANN - PAN-X~\cite{rahimi-etal-2019-%massively}\\
%  \bottomrule
%  \end{tabular}
%\end{table}

\begin{table}[H]
  \caption{List of Used Corpora - shows each corpus with an abbreviated name used in this paper, followed by the number of sentences, the number of tokens it contains, and lastly, its long name.}
  \label{tab:corpora}
  \begin{tabular}{lrrl}
    \toprule
    Corpus&Sentences&Tokens&Long Name\\
    \midrule
    \multicolumn{4}{c}{Slovenian} \\
    \midrule
    bsnlp&18106&400291&BSNLP 2017/21~\cite{piskorski-etal-2021-slav}\\
    500k&9483&193611&ssj500k 2.3~\cite{ssj500k-23}\\
    ewsd&2024&31233&{ELEXIS}-{WSD} 1.0~\cite{ELEXIS-WSD-10}\\
    scr&18139&391526&{SentiCoref} 1.0~\cite{SentiCoref-10}\\
    \midrule
    \multicolumn{4}{c}{Croatian} \\
    \midrule
    bsnlp&820&18704&BSNLP 2017 and 2021~\cite{piskorski-etal-2021-slav}\\
    500k&24780&504227&hr500k 1.0 ~ \cite{hr500k-10}\\
    \midrule
    \multicolumn{4}{c}{Serbian} \\
    \midrule
    set&3891&86726&{SETimes}.{SR} 1.0~\cite{SETimes-SR-1.0}\\
    \midrule
    \multicolumn{4}{c}{Bosnian} \\
    \midrule
    wann&8917&199378&WikiANN - PAN-X~\cite{rahimi-etal-2019-massively}\\
    \midrule
    \multicolumn{4}{c}{Macedonian} \\
    \midrule
    wann&16227&156467&WikiANN - PAN-X~\cite{rahimi-etal-2019-massively}\\
  \bottomrule
  \end{tabular}
\end{table}
We used NER tags in IOB2~\cite{IOB2} format from the CoNLL-2003 shared task~\cite{CoNLL2003} as a common denominator for all corpora and experiments.
Each corpus was first combined if split, then converted to a common format, reshuffled, and split to train/dev/test set in an 80/10/10 ratio.

%\mpu{might be nice to give an actual example of one or two sentences with NER tags. I know NER is a very standard task these days, but it always feels slightly uncomfortable to have a NLP paper where you don't show what the actual language data looks like, or what the task actually is}

We produced combined corpora by concatenating the sets without further reshuffling so that the experiments could be repeated.

The Slovene corpora were obtained from BSNLP, and parts of a newly published combined Training corpus {SUK} 1.0~\cite{SUK-1.0}
%Prepared and converted data is available on Github for convenience and possible manual review (move to online resources?).

\subsection{Data Conversion}
\label{subsec:data-conversion}
The first obstacle was the different NER tags used in corpora.
We decided to keep only common tags: PER, LOC, and ORG\@.
For example, the BSNLP corpus uses PRO and EVT tags, while the WikiANN corpus lacks a MISC tag common to ssj500k and hr500k training corpora.
All non-common tags, including MISC, were replaced with O (outside IOB).

The second obstacle was the difference in format.
BSNLP corpus, for instance, uses separate files for verbatim text and NER tags, with no positional reference between one another.
We used CLASSLA~\cite{ljubesic-dobrovoljc-2019-neural} sentence segmentation and tokenization with a custom conversion script to solve this problem.

The third step was to remove sentences longer than 128 tokens and convert corpora from standard CoNLL format to CSV format with two fields:
\begin{itemize}
  \item Sentence: whitespace separated sentence word tokens.
  \item NER: white space separated NER tags for each sentence word token.
\end{itemize}

\begin{table}[H]
  \caption{Example whitespace separated sentence word tokens with corresponding IOB2 NER tags.}
  \label{tab:ner_example}
  \begin{tabular}{llllll}
    \toprule
    Obtoženka&Asia&Bibi&zapustila&Pakistan\\
    \midrule
    O&B-PER&I-PER&O&B-LOC\\
    \bottomrule
  \end{tabular}
\end{table}
The fourth and final step was splitting the corpus data into train, test, and dev sets.

\subsection{Corpora analysis}
\label{subsec:corpora-analysis}
Comparing the corpora showed the differences that could potentially be problematic for obtaining aligned model performance.
Especially considering the NER tag ratios where the WikiANN automatically annotated corpora structure was standing out (see Table~\ref{tab:corpora_analysis} and Figure~\ref{fig:corpora_analysis}).
\begin{table}[H]
  \caption{Analysis of Combined Corpora - shows each language's combined corpora number of tokens per sentence, followed by the number of NER tags per token. Finally, the PER, LOC, and ORG columns show the ratios with respect to all NER tags.}
  \label{tab:corpora_analysis}
  \begin{tabular}{lrrrrr}
    \toprule
    Lang.&tok.\ per sent.&NER per tok.&PER&LOC&ORG\\
    \midrule
    sl&21.29&9.09\%&31.70\%&22.20\%&34.13\%\\
    hr&20.43&7.41\%&28.71\%&20.55\%&30.82\%\\
    sr&22.29&12.01\%&29.96\%&30.12\%&32.35\%\\
    bs&\textbf{7.81}&\textbf{36.91\%}&31.65\%&29.67\%&38.67\%\\
    mk&\textbf{9.64}&\textbf{28.07\%}&34.89\%&30.32\%&34.79\%\\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{wikiann-skew}
  \caption{WikiANN corpus skew}
  \label{fig:corpora_analysis}
\end{figure}

Fortunately, we were unable to detect any inconsistencies regarding performance measurements.

\section{Methods}
\label{sec:methods}
%\hl{Always try to add an introduction on what will be discussed in the following section. }
In the following section, we will introduce the hypothesis and discuss the methods used for testing it.

We hypothesize that the NER classification F1-score increases when we fine-tune the pre-trained multilingual model with an additional, related language.

%\mpu{there's no clear statement of what your objective is: what is the hypothesis you are testing, and how. This is probably the right place.}
\subsection{Method}
\label{subsec:method}
The selected method was first to select the pre-trained embeddings, train the baseline model for each language and produce NER classification measurements.
Baseline models were fine-tuned with only one - target language.
Next, we combined additional language corpora, re-trained the model, and again measured performance against the target language test set.

We used the Hugging Face's transformers Python library~\cite{wolf-etal-2020-transformers} for all the experiments.

\subsection{Measurements}
\label{subsec:measurements}
For all classification measurements, the Seqeval library~\cite{seqeval} was used.
Although the library uses CoNLL evaluation by default, we chose ``strict'' mode evaluation.
When calculating measurements, the strict mode also considers the IOB tag's ``beginning'' and ``inside'' parts.
Therefore the NER tags must match exactly.

\subsection{Parameters}
\label{subsec:parameters}
For all the experiments, we used the following hyper-parameters:
\begin{itemize}
  \item 256 max-length for tokenizer
  \item PyTorch's AdamW algorithm with 5e-5 learning rate
  \item batch size of 20
  \item 40 epochs
  \item F1-score for best model selection and training progression.
\end{itemize}



\section{Evaluation}
\label{sec:evaluation}
In the following section, we define the F1-score we used for evaluation, and then we present the experiment results: the evaluation of the pre-trained multilingual model, followed by the evaluation
of fine-tuning for each language.

\subsection{Evaluation measure}
For the evaluation of the classification models, we used the traditional F-measure or balanced F-score, which is the harmonic mean of precision and recall:
\begin{equation*}
    \text{F1-score} = 2  \cdot \frac{Precision \cdot Recall}{Precision +  Recall}
\end{equation*}
The Precision and Recall are defined as:
\begin{equation*}
\begin{split}
    \text{Precision} = \frac{TP}{TP + FP}
\end{split}
\quad\quad
\begin{split}
    \text{Recall} = \frac{TP}{TP + FN}
\end{split}
\end{equation*}
given that:
\begin{itemize}
  \item FP: a NER tag that is predicted but not present in the test.
  \item FN: a NER tag present in the test but missing in our prediction.
  \item TP: a NER tag that is correctly predicted.
\end{itemize}

%We used a target language test set for the evaluation and measured performance against the fine-tuned models.
The overall F1-score, used in the evaluation tables and figures, is a macro-averaged F1-score over all three NER tags.
Macro-averaged F1-score is computed using the arithmetic mean of all the per-class F1 scores:
\begin{equation*}
    \text{Macro-averaged F1-score} = \frac{1}{n}\sum_{i=1}^{n} F1_i
\end{equation*}
where $F1_i$ is the F1-score for $i-th$ NER tag.

The average distance from the baseline was used as a measure to show the overall variability of different models tested with the same test set.

Due to time and resource constraints, we excluded Bosnian and Macedonian language analysis.
Their corpora structure also influenced this decision, but we included them in evaluating other languages to see if something interesting would emerge.

\subsection{Pre-trained model selection}
\label{subsec:model-selection}
Prior work used two pre-trained multilingual models from Hugging Face (among others), which produced the best results:
\begin{itemize}
  \item BERT multilingual base model (cased)~\cite{DBLP:journals/corr/abs-1810-04805}
  \item XLM-RoBERTa (base sized model)~\cite{DBLP:journals/corr/abs-1911-02116}
\end{itemize}

Due to the task and resource limitations, we continued with our experiments using only XLM-RoBERTa base sized model
since the initial baseline tests showed that XLM-RoBERTa consistently outperforms the BERT model (see Table~\ref{tab:roberta_vs_bert}).

\begin{table}[H]
  \caption{XLM-RoBERTa vs. BERT multilingual cased}
  \label{tab:roberta_vs_bert}
  \begin{tabular}{llrrrr}
    \toprule
    Corpora&Precision&Recall&F1&Accuracy\\
    \midrule
    \multicolumn{5}{c}{BERT} \\
    \midrule
    bsnlp&0.935&0.931&0.933&0.990\\
    ssj500k&0.826&0.828&0.827&0.984\\
    bsnlp+ssj500k&0.898&0.903&0.900&0.986\\
    \midrule
    \multicolumn{5}{c}{XLM-RoBERTa} \\
    \midrule
    bsnlp&0.947&0.954&\textbf{0.951}&0.993\\
    ssj500k&0.869&0.902&\textbf{0.885}&0.990\\
    bsnlp+ssj500k&0.929&0.931&\textbf{0.930}&0.990\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Slovene}
\label{subsec:slovene}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{eval_sl}
  \caption{Slovene language test set model performance} %\hl{Sorting the bars in a barchart with respect of the dataset is not much informative - since we are visually asked to compare same colored bars that will always be spatially distant from one another. Consider grouping the bars by the metric rather than the dataset. }}
  \label{fig:eval_sl}
\end{figure}

The Slovene test set shows surprising model stability.
This stability comes, assumingly, from larger corpora compared to the others.
It might be that the quality of the corpora also plays a crucial role in this observation.

\begin{table}[H]
  \caption{Slovene language test set model performance}
  \label{tab:eval_sl}
  \begin{tabular}{lrrrrr}
    \toprule
    Model&PER F1&LOC F1&ORG F1&Overall F1\\
    \midrule
    baseline sl&0.963&0.963&0.931&0.952\\
    \midrule
    sl.sr&0.963&0.955&0.921&0.946\\
    sl.hr&0.962&0.96&0.924&0.948\\
    sl.hr.sr&0.964&0.958&0.925&0.949\\
    sl.hr.sr.bs&0.964&0.953&0.926&0.948\\
    sl.hr.sr.bs.mk&0.962&0.952&0.926&0.947\\
    \midrule
    avg.\ dist.&0.00071&0.00695&0.00634&0.00433\\
    \bottomrule
  \end{tabular}
\end{table}

If we observe the average distance from the baseline in the table's last row, we can see that it is only near 0.5\%.

\subsection{Croatian}
\label{subsec:croatian}

The Croatian language test set shows more significant variability when tested with different models.
We can observe the most significant variability on the ORG tag.
It might be that the other corpora training is influencing variability.
We can see that the average distance from the baseline is between 0.5\% and 1\%.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{eval_hr}
  \caption{Croatian language test set model performance}
  \label{fig:eval_hr}
\end{figure}

\begin{table}[H]
  \caption{Croatian language test set model performance}
  \label{tab:eval_hr}
  \begin{tabular}{llrrrr}
    \toprule
    Model&PER F1&LOC F1&ORG F1&Overall F1\\
    \midrule
    baseline hr&0.934&0.911&0.874&0.906\\
    \midrule
    hr.sr&0.932&0.921&0.888&0.914\\
    sl.hr&0.925&0.915&0.878&0.906\\
    hr.sr.bs&0.922&0.912&0.856&0.897\\
    sl.hr.sr&0.923&0.908&0.865&0.899\\
    sl.hr.sr.bs&0.938&0.927&0.873&0.912\\
    sl.hr.sr.bs.mk&0.925&0.911&0.861&0.899\\
    \midrule
    avg.\ dist.&0.00757&0.00554&0.00977&0.00624\\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Serbian}
\label{subsec:serbian}
The Serbian language test set is the only one that increased its performance measurements regarding its baseline.
Its average distance in performance measurements from the baseline is from approximately 0.5\% to 2.5\%.
The main suspect for this phenomenon is the Serbian corpora size. %\hl{this is an interesting insight, would you dare to draw a plot (or ranking) of the F1-Score divided by the LOGARITHM of the number of training instances per language, maybe we can support this claim strongly) }
It is the smallest included in this analysis and the most easily influenced by other corpora.
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{eval_sr}
  \caption{Serbian language test set model performance}
  \label{fig:eval_sr}
\end{figure}

\begin{table}[H]
  \caption{Serbian language test set model performance}
  \label{tab:eval_sr}
  \begin{tabular}{llrrrr}
    \toprule
    Model&PER F1&LOC F1&ORG F1&Overall F1\\
    \midrule
    baseline sr&0.962&0.979&0.914&0.954\\
    \midrule
    sl.sr&0.979&0.98&0.934&0.965\\
    hr.sr&0.987&0.988&0.956&0.978\\
    hr.sr.bs&0.982&0.987&0.945&0.973\\
    sl.hr.sr&0.979&0.979&0.946&0.969\\
    sl.hr.sr.bs&0.971&0.976&0.92&0.957\\
    sl.hr.sr.bs.mk&0.988&0.978&0.942&0.97\\
    \midrule
    avg.\ dist.&0.01885&0.00372&0.02603&0.01504\\
    \bottomrule
  \end{tabular}
\end{table}

\section{Conclusion}
\label{sec:conclusion}
We have shown that the model performance is not influenced substantially when trained with other than a target language.
%This assumption \mpu{it's not an `assumption' (something you must assume in order to do your experiment), but a `hypothesis' (something that your experiment tests)} seems true \mpu{`true' is also a dangerous word to use in science. Better to say the hypothesis has (or has not) been upheld} when we have enough training data for the target language.
%This hypothesis has been upheld.
Fine-tuning with other languages may benefit only the ``low resource'' languages.

Our initial hypothesis has not been upheld, but the result is still beneficial:
When using a multilingual model in an application, the performance would not degrade much if we fine-tune the model with additional language corpora from the same family.


\bibliographystyle{ACM-Reference-Format}
\bibliography{atlner}


\appendix

\section{Online Resources}
\label{sec:online-resources}

All the measurement results, including the ones not presented in this paper, together with code and corpora, can be found online at
{\color{blue}\href{https://github.com/ivlcic/trans-ner}{GitHub}}.
More user-friendly results are also available as a
{\color{blue}\href{https://docs.google.com/spreadsheets/d/16UaPR-qneNN8mlzjNkjc9kdry2eQl4J84t3EfDOtBj0/edit?usp=sharing}{Spreadsheet}}.

\end{document}
\endinput
